# 导入所需的库
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score
import matplotlib.pyplot as plt
import time

# 读取数据集
df = pd.read_csv("AllData.csv")

# 将数据集分成特征和标签
X = df.iloc[:, 1:].values  # 特征是用电量
y = df.iloc[:, 0].values  # 标签是是否窃电

# 将数据集分成训练集和测试集，比例为8:2
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 将数据转换成张量格式
X_train = torch.from_numpy(X_train).float()
X_test = torch.from_numpy(X_test).float()
y_train = torch.from_numpy(y_train).long()
y_test = torch.from_numpy(y_test).long()


# 定义卷积神经网络模型的类
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # 定义第一个卷积层，输入通道为1，输出通道为16，卷积核大小为3，步长为1，填充为1
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
        # 定义第一个池化层，使用最大池化，池化核大小为2，步长为2
        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)
        # 定义第二个卷积层，输入通道为16，输出通道为32，卷积核大小为3，步长为1，填充为1
        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        # 定义第二个池化层，使用最大池化，池化核大小为2，步长为2
        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)
        # 定义全连接层，输入维度为32*259，输出维度为2（二分类问题）
        self.fc = nn.Linear(32 * 259, 2)

    def forward(self, x):
        # 将输入数据x变成(N, 1, 1036)的形状，N是批量大小
        x = x.view(-1, 1, 1036)
        # 通过第一个卷积层和激活函数（使用ReLU）
        x = F.relu(self.conv1(x))
        # 通过第一个池化层
        x = self.pool1(x)
        # 通过第二个卷积层和激活函数（使用ReLU）
        x = F.relu(self.conv2(x))
        # 通过第二个池化层
        x = self.pool2(x)
        # 将数据展平成(N, 32*259)的形状
        x = x.view(-1, 32 * 259)
        # 通过全连接层和激活函数（使用softmax）
        x = F.softmax(self.fc(x), dim=1)
        return x


# 定义联邦学习的FedAvg算法的函数
def FedAvg(w):
    # w是一个列表，包含了所有客户端的模型参数（字典格式）
    # 返回值是一个字典，表示所有客户端模型参数的平均值

    # 获取客户端的数量
    m = len(w)
    # 获取模型参数的名称和形状
    params = w[0].keys()
    shapes = {k: w[0][k].shape for k in params}
    # 初始化一个空字典，用于存储平均后的模型参数
    w_avg = {}
    # 对于每一个模型参数，计算所有客户端的平均值，并存入w_avg中
    for k in params:
        w_avg[k] = torch.zeros(shapes[k])
        for i in range(m):
            w_avg[k] += w[i][k]
        w_avg[k] /= m
    # 返回平均后的模型参数
    return w_avg


# 定义训练函数，输入是一个客户端的数据和一个模型，输出是训练后的模型参数和损失值
def train(X, y, model):
    # 定义损失函数，使用交叉熵损失
    criterion = nn.CrossEntropyLoss()
    # 定义优化器，使用随机梯度下降法，学习率为0.01
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    # 定义训练的批量大小为32
    batch_size = 32
    # 定义训练的轮数为10
    epochs = 10
    # 获取数据的总量
    n = len(X)
    # 初始化一个空列表，用于存储每一轮训练的平均损失值
    losses = []
    # 对于每一轮训练
    for epoch in range(epochs):
        # 打乱数据的顺序
        perm = torch.randperm(n)
        X = X[perm]
        y = y[perm]
        # 初始化一个变量，用于累计每一批次的损失值
        running_loss = 0.0
        # 对于每一个批次
        for i in range(0, n, batch_size):
            # 获取批次的数据和标签
            inputs = X[i:i + batch_size]
            labels = y[i:i + batch_size]
            # 清零梯度
            optimizer.zero_grad()
            # 前向传播，计算模型的输出
            outputs = model(inputs)
            # 计算损失值
            loss = criterion(outputs, labels)
            # 反向传播，更新模型参数
            loss.backward()
            optimizer.step()
            # 累加损失值
            running_loss += loss.item()
        # 计算并存储每一轮训练的平均损失值
        losses.append(running_loss / (n // batch_size))

    # 返回训练后的模型参数和损失值列表
    return model.state_dict(), losses


# 定义测试函数，输入是测试集数据和一个模型，输出是测试集上的准确率和AUC值
def test(X, y, model):
    # 将模型设置为评估模式（不使用dropout等）
    model.eval()
    # 获取数据的总量
    n = len(X)
    # 初始化一个变量，用于累计预测正确的数量
    correct = 0
    # 初始化一个空列表，用于存储预测的概率值
    probs = []
    # 对于每一个数据点
    for i in range(n):
        # 获取输入数据和标签
        input = X[i].unsqueeze(0)
        label = y[i].item()
        # 计算模型的输出（概率分布）
        output = model(input)
        # 获取预测的类别（概率最大的那个）
        pred = output.argmax().item()
        # 如果预测正确，累加正确数量
        if pred == label:
            correct += 1
        # 将预测的概率值（属于窃电类别的概率）存入列表中
        probs.append(output[0][1].item())

    # 计算并返回准确率和AUC值
    acc = correct / n
    auc = roc_auc_score(y, probs)
    return acc, auc


# 定义联邦学习的主函数，输入是训练集数据和测试集数据，输出是最终的模型参数和各种评估指标
def federated_learning(X_train, y_train, X_test, y_test):
    # 定义客户端的数量为10
    m = 10
    # 将训练集数据平均分配给每个客户端
    X_train_split = torch.chunk(X_train, m)
    y_train_split = torch.chunk(y_train, m)
    # 初始化一个空列表，用于存储每个客户端的模型参数
    w = []
    # 初始化一个空列表，用于存储每一轮联邦学习的平均损失值
    losses = []
    # 初始化一个空列表，用于存储每一轮联邦学习的测试集准确率
    accs = []
    # 初始化一个空列表，用于存储每一轮联邦学习的测试集AUC值
    aucs = []
    # 初始化一个空列表，用于存储每一轮联邦学习的测试集MAP值
    maps = []
    # 初始化一个变量，用于记录联邦学习的开始时间
    start_time = time.time()
    # 对于每一轮联邦学习（假设进行20轮）
    for round in range(20):
        # 打印当前的轮数
        print(f"Round {round + 1}")
        # 清空w列表
        w.clear()
        # 对于每一个客户端
        for i in range(m):
            # 创建一个新的模型实例
            model = CNN()
            # 调用训练函数，传入该客户端的数据和模型，得到训练后的模型参数和损失值
            wi, lossi = train(X_train_split[i], y_train_split[i], model)
            # 将训练后的模型参数存入w列表中
            w.append(wi)
            # 打印该客户端的训练损失值
            print(f"Client {i + 1} loss: {lossi[-1]:.4f}")
        # 调用FedAvg函数，传入w列表，得到平均后的模型参数
        w_avg = FedAvg(w)
        # 创建一个新的模型实例
        model = CNN()
        # 将平均后的模型参数加载到模型中
        model.load_state_dict(w_avg)
        # 调用测试函数，传入测试集数据和模型，得到测试集上的准确率和AUC值
        acc, auc = test(X_test, y_test, model)
        # 计算并得到测试集上的MAP值（使用sklearn库中的average_precision_score函数）
        map = average_precision_score(y_test, model(X_test)[:, 1])
        # 将平均损失值，准确率，AUC值和MAP值分别存入相应的列表中
        losses.append(sum(lossi) / len(lossi))
        accs.append(acc)
        aucs.append(auc)
        maps.append(map)
        # 打印测试集上的各种评估指标
        print(f"Test accuracy: {acc:.4f}")
        print(f"Test AUC: {auc:.4f}")
        print(f"Test MAP: {map:.4f}")

    # 计算并打印联邦学习的总耗时（单位为秒）
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Total time: {elapsed_time:.2f} seconds")

    # 返回最终的模型参数和各种评估指标列表
    return w_avg, losses, accs, aucs, maps


# 调用联邦学习的主函数，传入训练集数据和测试集数据，得到最终的模型参数和各种评估指标列表
w, losses, accs, aucs, maps = federated_learning(X_train, y_train, X_test, y_test)

# 使用matplotlib库绘制损失值，准确率，AUC值和MAP值随轮数变化的折线图
plt.figure(figsize=(12, 8))
plt.subplot(2, 2, 1)
plt.plot(losses)
plt.xlabel("Round")
plt.ylabel("Loss")
plt.title("Loss vs Round")
plt.subplot(2, 2, 2)
plt.plot(accs)
plt.xlabel("Round")
plt.ylabel("Accuracy")
plt.title("Accuracy vs Round")
plt.subplot(2, 2, 3)
plt.plot(aucs)
plt.xlabel("Round")
plt.ylabel("AUC")
plt.title("AUC vs Round")
plt.subplot(2, 2, 4)
plt.plot(maps)
plt.xlabel("Round")
plt.ylabel("MAP")
plt.title("MAP vs Round")
plt.tight_layout()
plt.show()
